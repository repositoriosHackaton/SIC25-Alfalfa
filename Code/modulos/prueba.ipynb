{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import joblib\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# preprocesado\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    stop_words = set(stopwords.words('english'))  # English stopwords\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# preprocesado\n",
    "def preprocess_data(df, text_column='text'):\n",
    "    df[text_column] = df[text_column].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "# binarizar \n",
    "def binarize(df,label_column = 'label'):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    labels = mlb.fit_transform(df[label_column])\n",
    "    labels_df = pd.DataFrame(labels, columns=mlb.classes_)\n",
    "    return labels_df, mlb\n",
    "\n",
    "# tokenizar\n",
    "def tokenize_data_in_batches(tokenizer, texts, max_length=128, batch_size=32):\n",
    "    encodings = {'input_ids': [], 'attention_mask': []}\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_encodings = tokenizer(\n",
    "            batch, padding=True, truncation=True, max_length=max_length, return_tensors='tf'\n",
    "        )\n",
    "        encodings['input_ids'].append(batch_encodings['input_ids'])\n",
    "        encodings['attention_mask'].append(batch_encodings['attention_mask'])\n",
    "    encodings['input_ids'] = tf.concat(encodings['input_ids'], axis=0)\n",
    "    encodings['attention_mask'] = tf.concat(encodings['attention_mask'], axis=0)\n",
    "    return encodings\n",
    "\n",
    "# crear dataset tf\n",
    "def create_tf_datasets(train_encodings, y_train, val_encodings, y_test, batch_size=16):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).shuffle(1000).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_test)).batch(batch_size)\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# CARGAR MODELO PRE ENTRENADO Y TOKENIZADOR\n",
    "def create_bert(model_name= 'bert-base-uncased'):\n",
    "    model = TFBertModel.from_pretrained(model_name)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Dividir datos\n",
    "def split_data(df, labels_df, test_size=0.2, random_state=42):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        df['text'], labels_df, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "#OPCIONAL\n",
    "def tune_bert(train_dataset, val_dataset, model_name='bert-base-uncased', num_labels=None, epochs=5):\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, callbacks=[early_stopping])\n",
    "    return model\n",
    "\n",
    "# CREAR RED CONVOLUCIONADA\n",
    "def create_cnn(bert_model, num_labels):\n",
    "    input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name='attention_mask')\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)\n",
    "    x = bert_output.last_hidden_state\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(filters=128,kernel_size=3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters = 64, kernel_size=3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(num_labels, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Entrenar BERT + CNN\n",
    "def train_mixed_model(model,train_dataset,val_dataset, epochs= 10, patience = 3):\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',patience=patience)\n",
    "    history = model.fit(train_dataset,validation_data = val_dataset, epochs=epochs, callbacks=[early_stopping])\n",
    "    return history\n",
    "\n",
    "# funcion del profe :)\n",
    "def categorizar_articulo_subtemas_en(texto, palabras_clave, vectorizer, umbrales, top_n=3):\n",
    "    try:\n",
    "        texto = texto.lower()\n",
    "        texto = re.sub(r'[^\\w\\s]', '', texto)  # remover puntuacion\n",
    "        stop_words = set(stopwords.words('english'))  # English stopwords\n",
    "        palabras = [palabra for palabra in texto.split() if palabra not in stop_words]\n",
    "\n",
    "        # Lemmatizacion\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        palabras = [lemmatizer.lemmatize(palabra) for palabra in palabras]\n",
    "\n",
    "        texto_limpio = \" \".join(palabras)  \n",
    "        vector_texto = vectorizer.transform([texto_limpio])  # Convertir texto a vector\n",
    "\n",
    "        \n",
    "        similitudes = {}\n",
    "        for subtema, palabras_clave_subtema in palabras_clave.items():\n",
    "            vector_palabras_clave = vectorizer.transform([\" \".join(palabras_clave_subtema)])\n",
    "            similitudes[subtema] = cosine_similarity(vector_texto, vector_palabras_clave)[0][0]\n",
    "\n",
    "        # organizar por similaridad\n",
    "        categorias_ordenadas = sorted(similitudes.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # seleccionar top categorias\n",
    "        top_categorias = []\n",
    "        for categoria, similitud in categorias_ordenadas:\n",
    "            umbral_categoria = umbrales.get(categoria, 0.1)  \n",
    "            if similitud >= umbral_categoria:\n",
    "                top_categorias.append(categoria)\n",
    "\n",
    "        if not top_categorias:\n",
    "            top_categorias.append(\"No specific subtopic\")\n",
    "\n",
    "        return top_categorias[:top_n]  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el texto: {e}\")\n",
    "        return [\"Error\"]  \n",
    "    \n",
    "\n",
    "\n",
    "def evalua_model(model, val_dataset, mlb):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch in val_dataset:\n",
    "        X, y = batch\n",
    "        y_true.extend(y.numpy())\n",
    "        y_pred.extend(model.predict(X).numpy())\n",
    "    \n",
    "    y_true = mlb.inverse_transform(y_true)\n",
    "    y_pred = mlb.inverse_transform(y_pred)\n",
    "    \n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of main.py\n",
    "def main():\n",
    "    try:\n",
    "        df = pd.read_csv(\"labelled_data2.csv\", encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(\"other_data.csv\", encoding='latin-1')  # encoding alternativo\n",
    "    except FileNotFoundError:\n",
    "        print(\"File 'labelled_data2.csv' not found.\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "        df = preprocess_data(df)\n",
    "        \n",
    "        labels_df, mlb = binarize(df)\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = split_data(df, labels_df)\n",
    "        \n",
    "        bert_model, tokenizer = create_bert()\n",
    "        \n",
    "        train_encodings = tokenize_data_in_batches(tokenizer, x_train)\n",
    "        \n",
    "        val_encodings = tokenize_data_in_batches(tokenizer, x_test)\n",
    "\n",
    "        train_dataset, val_dataset = create_tf_datasets(train_encodings, y_train, val_encodings, y_test)\n",
    "        \n",
    "        tuned_bert = tune_bert(train_dataset, val_dataset,num_labels=labels_df.shape[1])\n",
    "        # Si necesitamos el fine tuning del bert podemos pasarlo como parametro aca\n",
    "        cnn_model = create_cnn(bert_model, num_labels=labels_df.shape[1])\n",
    "        \n",
    "        history = train_mixed_model(cnn_model, train_dataset, val_dataset)\n",
    "        \n",
    "        evalua_model(cnn_model,val_dataset,mlb)\n",
    "\n",
    "        cnn_model.save('bert_cnn_cybersecurity_model.h5')\n",
    "        joblib.dump(mlb, 'mlb.pkl')\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
